# On-Policy Distillation：模型训练的第三条道路，极致性价比

## 背景
首先，on-policy distillation 并不是一个新概念，早在 2015 年就有论文提出了这个概念。

为什么引起广泛关注：
> 到 2025 年 2 月启动时，有报道称 Thinking Machines Lab 已从包括 OpenAI、Meta AI 和 Mistral AI 在内的竞争对手处雇佣了约 30 名研究人员和工程师。 [6] [7] [8] [9] [10] 其创始团队成员包括巴雷特·佐夫（Barret Zoph），前 OpenAI 研究副总裁（后训练）、莉莲·翁（Lilian Weng），前 OpenAI 副总裁，以及 OpenAI 联合创始人约翰·舒尔曼（John Schulman），他在短暂加入该实验室竞争对手 Anthropic 后加入了该实验室。 [11] [12] 其他被雇佣的前 OpenAI 员工包括乔纳森·拉克曼（Jonathan Lachman）和安德鲁·塔洛克（Andrew Tulloch）（尽管塔洛克在被 Meta 超级智能实验室挖走后已离职）。 [13] [14] Thinking Machines Lab 的顾问包括此前担任 OpenAI 首席研究官的鲍勃·麦格鲁（Bob McGrew）和曾任 OpenAI 首席研究员的亚历克·拉德福德（Alec Radford）。 [15] [16]

>在 2025-10-01，该实验室宣布了 Tinker，一款用于微调语言模型的 API。用户将通过该 API 提交作业以微调受支持的各种开放权重模型之一。该实验室将在其内部集群和训练基础设施上运行这些作业。 [17]

前段时间出了一篇写LLM推理不确定的博文，感兴趣的可以看看：https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/

> 当学生从自己的错误中学习，同时接受大师对每一步的精准指导，这就是on-policy distillation的精髓。

## 🎯 问题的背景：模型“后训练”的困境
文章指出，训练LLM分为三个阶段：预训练、中训练和后训练。后训练的目的是教会模型特定的行为（如遵循指令或聊天）。现有的后训练方法主要有两种，但各有缺陷：

1. ** Off-policy**:
- **方式**: 让“学生”模型模仿“教师”模型（一个更强的大模型）生成的优质范例。
- **优点**: 提供了“密集”的反馈信号（每一步都有正确答案）。
- **缺点**: 学生模型是在“教师”的舒适区学习。一旦学生在实际使用中犯了一个“教师”从未犯过的错误，它就不知道如何纠正，导致“复合错误” (compounding error)。

1. ** On-policy 训练**:
- **方式**: 让“学生”模型自己探索，并根据其最终表现（例如答案是否正确）给予奖励或惩罚。
- **优点**: 学生能从自己的错误中学习，相关性更强。
- **缺点**: 反馈信号非常“稀疏” (sparse)，效率极低。模型可能知道自己答错了，但不知道错在哪一步。

讨论：
1. SFT是off-policy还是on-policy？
2. GRPO类RL是off-policy还是on-policy？

### 例子
![01](img/01.png)

![02](img/02.png)

强化学习有一个主要缺点：它提供的反馈非常稀疏，每次训练回合传递的比特数是固定的，与所用的标记数量无关。在上面的例子中，学生学会了“21”是错误答案并朝着不再产生它的方向更新。但它并不知道错误究竟出在哪里，是运算顺序弄错了，还是算术本身出错了。这种反馈的稀疏性使得强化学习在许多应用中效率低下。

离策略训练通常以监督微调（SFT）的方式进行：在精心策划的任务特定标注样本集上训练。这些标注样本的来源可以是已被证明在该任务上表现良好的教师模型。

![03](img/03.png)

### 关键点
什么是蒸馏？
我们可以使用一种称为蒸馏的机制：训练学生模型去匹配教师模型的输出分布。我们在教师轨迹上训练：生成的完整序列，包括中间的思考步骤。我们可以在每一步使用教师的完整下一个标记分布（通常称为“logit 蒸馏”），也可以仅对给定序列进行采样。实际上，采样序列提供了对教师分布的无偏估计，并且到达相同的目标。

我们希望将强化学习的 on-policy 相关性与蒸馏的密集奖励信号结合起来。对于学棋来说，这相当于有一位老师对你每一步棋进行评分，从“严重失误”到“精彩绝伦”。对于 LLM 的后期训练，这就是 on-policy 蒸馏。

思考：
感觉类似过程奖励，和这个蒸馏策略的区别是什么？

## 💡 解决方案：策略蒸馏 (On-Policy Distillation)

| Method	| Sampling	| Reward signal |
|---------|---------|---------|
| Supervised finetuning	| off-policy	| dense |
| Reinforcement learning	| on-policy	| sparse |
| On-policy distillation	| on-policy	| dense |

策略蒸馏被誉为“两全其美”的方法：
- 它像 RL 一样采用“策略上” (on-policy) 采样： 轨迹（例如解题步骤）是由学生模型自己生成的。
- 它像 SFT 一样提供“密集” (dense) 反馈： 使用一个强大的教师模型来评估学生生成的轨迹，但不是只给一个总分，而是对每一个词元 (token) 进行逐一评分。
![04](img/04.png)

这样，学生模型就能在自己实际会遇到的情境中，获得关于每一步的即时、密集的指导，学会如何从自己的错误中恢复。

那直接用教师不就行了？为什么还要用学生？
经过更强训练的较小模型在其训练的专业领域中通常胜过更大的一般性模型。使用较小模型有许多好处：它们可以出于隐私或安全考虑在本地部署、可以更容易地持续训练和更新，并节省推理成本。要利用这些优势，需要为训练的后期阶段选择合适的方法。

### 实验


使用 Tinker 训练 API，我们复现了 Qwen3 的结果：通过在策略蒸馏以远低于强化学习成本的代价实现了在推理基准上相当的性能。
### 实现
损失函数：reverse KL
![05](img/05.png)

- 当学生的行为与教师完全相同时，反向 KL 为零
- 学生只优化下一个立即生成的词元，不考虑未来的词元。
![06](img/06.png)
![07](img/07.png)

这种方法显著节省计算资源。由于它不需要等到一次完整采样结束才能计算奖励，我们可以在训练时使用较短或部分的轨迹。查询教师模型的对数概率也只需对较大的模型进行一次前向传播，而轨迹由更小且更廉价的学生模型生成。

我们也不需要单独的奖励或标注模型。将基于蒸馏的逐标记（per-token）奖励与序列级别的环境奖励相结合可能具有优势；这是一个有趣且值得未来研究的方向。
![19](img/19.png)

#### Distillation for reasoning
1. Off-policy distillation
![08](img/08.png)



2. RL
| Method	| AIME’24	| GPQA-Diamond	| GPU Hours |
| Off-policy distillation	| 55.0%	| 55.6%	| Unreported |
| Reinforcement learning	| 67.6%	| 61.3%	| 17,920 |
| On-policy distillation	| 74.4%	| 63.3%	| 1,800 |

Qwen 团队还报告称，通过在策略蒸馏下以 RL 十分之一的成本在 AIME’24 上达到更高的 74.4 分，这成为我们工作的灵感来源。我
#### On-policy distillation
从 40 万步骤的 SFT 检查点开始，在大约 150 步内，on-policy 蒸馏在 AIME’24 上达到了 70%。
| Method | AIME'24 | Teacher FLOPs | Student FLOPs | CE vs SFT-2M |
|--------|---------|---------------|---------------|--------------|
| Initialization: SFT-400K | 60% | 8.5 × 10²⁰ | 3.8 × 10²⁰ | – |
| SFT-2M (extrapolated) | ~70% (extrapolated) | 3.4 × 10²¹ | 1.5 × 10²¹ | 1× |
| Reinforcement learning | 68% | - | - | ≈1× |
| On-policy distillation | 70% | 8.4 × 10¹⁹ | 8.2 × 10¹⁹ | 9-30× |


![09](img/09.png)
#### Training an internal assistant
除了将小模型训练到在常见任务上具备高性能之外，蒸馏的另一个用例是个性化。例如在对话中遵循特定语气和输出格式，或具备像工具使用和成本预算这样的能力。我们通常希望将这种行为与新的领域知识结合起来进行训练。
- 既要在某个领域拥有专业知识
- 同时还要具备可靠的助手机能
如何评价
- 模型对该领域（公司文档）有知识性。预训练模型未见过任何公司的内部文档，因此无论模型规模如何，只能猜测。我们将使用内部知识回忆评估（“内部问答”）来测量这一点。

- 该模型在后训练阶段表现出强大的行为能力，即遵循指令。我们将使用常用的 IF-eval 对此进行测量。

**在新知识上训练会削弱已学到的行为，也就是灾难性遗忘。**

![10](img/10.png)
![11](img/11.png)


LoRA 学得更少、遗忘更少


在策略蒸馏中恢复训练后行为

使用较早版本的模型作为教师，在微调过程中“重新唤回”丢失的能力，使得在策略蒸馏在持续学习方面非常有前景。我们可以在对新数据进行微调和通过蒸馏恢复行为之间交替进行，从而使我们的模型随着时间学习并保持知识的更新。
### 实验结果（知识与行为评估）

| 模型 | Internal QA Eval（知识） | IF-eval（对话） |
|------|-------------------------:|----------------:|
| Qwen3-8B | 18% | 85% |
| + 中期训练（100% 文档） | 43% | 45% |
| + 中期训练（70% 文档） | 36% | 79% |
| + 中期训练（70% 文档） + On-Policy 蒸馏 | 41% | 83% |

## 讨论
- 密集监督大大提高了算力效率
- 蒸馏可以有效重用训练数据以提高数据效率，on-policy 蒸馏通过最小化反向 KL 来学习近似教师模型的完整分布，而不是记住单一答案。这使我们能够对来自同一提示的多个样本进行训练。
- 强化学习在语义策略的空间中进行搜索
- 作为持续学习工具

## 道理都懂，代码呢？
https://huggingface.co/spaces/HuggingFaceH4/on-policy-distillation

### 奖励函数最小化reverse KL

## 📊 关键发现与应用案例
文章通过两个实验证明了策略蒸馏的有效性：

1. 用于数学推理:

- **与SFT（策略外）相比，策略蒸馏的计算效率高出 9-30 倍。**

- **与RL（策略上）相比，它仅用大约 1/10 的计算成本就达到了更高的数学推理性能（复现了Qwen3的技术报告）。**

2. 用于个性化和持续学习 (Personalization / Continual Learning):

- **问题**: 当一个模型（如内部助理）学习了新知识（例如公司内部文件）后，它会“忘记”之前学到的技能（如遵循指令的能力），这被称为“灾难性遗忘”。

- **结果**: 在模型学习了新知识后，使用策略蒸馏（以原始模型为教师）可以成功恢复其被遗忘的聊天和指令遵循能力，同时不丢失新学到的知识。



## 引言：一个优雅的权衡

在机器学习领域，有一个长期困扰研究者的基本矛盾：**样本效率**与**数据相关性**之间的取舍。传统的强化学习方法（on-policy RL）虽然让智能体从自身经验中学习、能直接纠正自己的错误，但反馈稀疏、训练成本高昂；而离线蒸馏（off-policy distillation）虽然高效、反馈密集，却存在训练分布与实际使用分布不匹配的问题——学生只见过教师走访的情境，面对自身错误时束手无策。

On-Policy Distillation的出现，优雅地解决了这一困境。它将两种方法的优势巧妙结合：学生模型使用**自身策略**与环境交互（on-policy的相关性），同时由强大的教师模型对**每一步行为**给予密集反馈（蒸馏的高信息量）。如果用围棋来类比：传统RL如同自己下完整盘才知胜负；离线蒸馏像是背诵大师棋谱却不知如何应对变化；而on-policy distillation则是**让大师站在身后，对你下出的每一步棋即时点评**。

## 第一章：理论根基——两大思想支柱的交汇

### 1.1 知识蒸馏：从"暗知识"到现代架构

知识蒸馏由Hinton等人在2015年正式提出，核心理念是将庞大教师模型的知识迁移到小型学生模型中。其创新之处在于使用**"软目标"（soft targets）**——教师模型输出的完整概率分布，而非传统的"硬标签"。

考虑一个图像分类任务：硬标签只告诉你"这是猫"，但软目标会告诉你："猫（90%）、狗（7%）、老虎（2%）、汽车（0.01%）"。这个概率分布蕴含了丰富的"暗知识"——它揭示了类别间的相似性关系。通过引入温度参数$T$来平滑分布：

$$p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$$

当$T > 1$时，分布变得更"软"，低概率类别也能获得一定权重，让学生学会更细腻的区分。

**关键洞察**：知识蒸馏的真正价值不仅在于模型压缩，更在于发现了一种**信息密度极高的监督信号**。这为后来在强化学习中让教师策略有效指导学生奠定了理论基础。

### 1.2 强化学习的策略二分法：在策略 vs. 离策略

强化学习算法根据学习数据的来源分为两大阵营：

- **在策略学习（On-Policy）**：智能体从当前正在执行的策略产生的数据中学习。经典算法如A2C、PPO。
  - **优势**：经验高度相关，学习稳定
  - **劣势**：样本效率低——每次策略更新后，旧数据必须丢弃

- **离策略学习（Off-Policy）**：可以利用不同策略产生的数据。经典算法如DQN、SAC。
  - **优势**：样本效率高——通过经验回放反复利用历史数据
  - **劣势**：行为策略与目标策略的分布不匹配可能导致不稳定

这种分野体现了学习哲学的根本差异：是从"亲身经验"中学（on-policy），还是从"多样化历史数据"中学（off-policy）？前者如同学生做自己出的模拟题，针对性强但覆盖有限；后者如同学习历年真题集，数据丰富但可能无法精准解决当前薄弱环节。

**核心困境**：我们既需要经验的相关性，又需要数据的多样性与效率。On-Policy Distillation正是为破解这一困境而生。

### 1.3 历史渊源：从Policy Distillation到DAgger

**2015年：策略蒸馏的诞生**
Rusu等人首次将知识蒸馏应用于强化学习，成功将DQN的策略提炼到规模仅7-15%的小网络中，在Atari游戏上达到相当甚至更好的性能。然而，这本质上是**离策略蒸馏**——学生在教师访问过的状态分布上训练，面临经典的**分布失配（distributional shift）**问题。

**关键缺陷**：教师作为专家，其轨迹集中在"优质"状态的狭窄分布上。学生因不完美会犯错，进入教师从未经历的未知状态，导致连锁错误。这直接催生了后续在策略方法的需求。

**2011年：DAgger算法的启示**
虽然比策略蒸馏更早，DAgger（Dataset Aggregation）算法却是on-policy思想的重要先驱。其核心流程是：
1. 用初始专家数据训练学生策略
2. 让学生策略执行，收集学生实际访问的状态
3. 向专家查询这些状态下的正确动作
4. 将（学生状态，专家动作）加入数据集
5. 重新训练，循环迭代

DAgger的精髓在于：**专家为学生实际遇到的状态提供纠正性标签**，强制学生学会从自己的错误中恢复。这与On-Policy Distillation不谋而合：学生生成轨迹 → 教师为该轨迹的每个token提供密集反馈 → 学生更新策略。

**2017年：AlphaGo Zero的自我蒸馏**
AlphaGo Zero提出不使用人类数据、纯自我对弈提升的算法。每一步通过蒙特卡洛树搜索（MCTS）改进策略，然后训练网络模仿MCTS得到的强化策略。这可视为一种**on-policy策略蒸馏**：网络从自己对弈（on-policy）的局面中学习，更强的搜索策略充当教师。

## 第二章：现代范式——On-Policy Distillation的核心机制

### 2.1 概念突破：两全其美的设计

On-Policy Distillation巧妙地融合了两种范式的优势：

**① 在策略相关性（On-Policy Relevance）**
- 从学生模型$\pi_\theta$自身采样轨迹
- 确保学习数据来自学生实际访问的状态分布
- 直接解决分布失配问题
- 学生学会从**自身错误**中恢复

**② 密集蒸馏信号（Dense Distillation Signal）**
- 教师模型$\pi_T$为学生生成的**每一个token**提供反馈
- 完美解决信用分配（credit assignment）难题
- 精确告诉学生在推理链的**哪一环节**出错
- 信息量远超传统RL的稀疏终局奖励

**生动类比**：传统RL让学生下完整盘棋才告诉胜负；离策略蒸馏强迫学生背诵大师棋谱；On-Policy Distillation则是**学生自己下棋，大师站在身后对每一步即时打分**——既保证练习的相关性（针对学生自己的问题），又提供即时、具体、信息量极大的反馈。

### 2.2 算法框架：简洁而高效的三步流程

**Step 1: 轨迹生成（Trajectory Generation）**
学生模型$\pi_\theta$根据当前策略自回归采样生成token序列：
$$x_{1:T} \sim \pi_\theta$$

**Step 2: 教师评估（Teacher Evaluation）**
固定参数的教师模型$\pi_T$对学生生成序列中的每个token $x_t$进行评估，计算其在给定前文$x_{<t}$条件下的概率$\pi_T(x_t | x_{<t})$。关键：这只需教师模型一次前向传播，计算开销可控。

**Step 3: 损失计算与更新（Loss Calculation & Update）**
学生策略更新目标是最小化其在所访问状态上的概率分布与教师分布的差异。实践中最常用且最有效的损失函数是**逐token的反向KL散度（reverse KL divergence）**：

$$\mathcal{L}_{\text{on-policy}} = \mathbb{E}_{x_{1:T} \sim \pi_\theta} \sum_{t=1}^T D_{\text{KL}}(\pi_\theta(\cdot | x_{<t}) || \pi_T(\cdot | x_{<t}))$$

其中，$D_{\text{KL}}(P||Q) = \sum_x P(x)\log \frac{P(x)}{Q(x)}$。

### 2.3 反向KL的选择：为何"模式寻求"至关重要

选择**反向KL散度**而非前向KL散度，是算法的关键设计：

- **前向KL**（$D_{\text{KL}}(Q||P)$）是"均值寻求"（mean-seeking）：鼓励学生覆盖教师的所有可能模式，可能导致学生学到模糊、平均化的次优策略。

- **反向KL**（$D_{\text{KL}}(P||Q)$）是**"模式寻求"（mode-seeking）**：严厉惩罚学生在教师概率很低的地方赋予高概率的行为，迫使学生将概率质量集中在教师识别的最优模式上。

**为何这很重要**？在需要精确、逐步推理的任务（如数学推理）中，这种"模式寻求"特性对于惩罚那些导致推理偏离正轨的"分叉token"至关重要。

**实例说明**：考虑数学题"5 + (2 × 3) = ?"

学生错误轨迹：`5 + 2 is 7, and 7 × 3 is 21.`
教师对每个token的条件概率评分：
- `5` (40%) `+` (80%) `2` (5%) `is` (15%) `7` (99%) `,` (99%) ...

颜色越深表示学生越不可能生成该token，因此反向KL惩罚越大。教师精准识别出`2`这个"分叉token"——正是这一步将学生引向了错误的运算顺序。

### 2.4 范式对比：On-Policy Distillation的独特定位

| 特征 | 离策略蒸馏（SFT） | 在策略RL | On-Policy Distillation |
|------|------------------|---------|----------------------|
| 数据来源 | 教师轨迹（固定离线数据） | 学生轨迹 | **学生轨迹** |
| 监督信号 | 密集（逐token教师概率） | 稀疏（终局/环境奖励） | **密集（逐token教师概率）** |
| 分布失配风险 | 高（学生遇未见状态） | 低（在自身分布训练） | **低（在自身分布训练）** |
| 样本效率 | 高（可重用静态数据） | 低（策略更新后数据需丢弃） | **极高（在相关数据上施加密集信号）** |
| 主要挑战 | 状态失配导致复合误差 | 稀疏奖励下的信用分配 | 教师偏见；学生rollout质量低 |
| 核心用例 | 初始策略模仿 | 针对特定奖励函数优化 | **高效策略微调与对齐** |

**核心洞察**：On-Policy Distillation在学生自身生成的、最具相关性的数据上施加最密集的监督信号，实现了前所未有的学习效率。

## 第三章：实证验证——从理论到实践的跨越

### 3.1 数学推理：9-30倍的成本效率提升

**实验设置**：使用On-Policy Distillation训练Qwen3-8B-Base模型进行数学推理，教师模型为Qwen3-32B。

**中期训练（Mid-training）**：
首先在OpenThoughts-3数据集（由QwQ-32B生成的推理样本）上进行400k prompts的离线蒸馏（SFT）：
- 全量微调达到**60%** AIME'24得分
- 性能增长遵循对数线性规律
- 外推至70%约需**2M prompts**

**后训练对比**（从60%提升至70%）：

| 方法 | AIME'24得分 | 计算成本 | 成本效率比 |
|------|------------|---------|-----------|
| SFT-2M（外推） | ~70% | 1× | 基准 |
| RL（Qwen3报告） | 67.6% | 17,920 GPU小时 | ≈1× |
| **On-Policy Distill** | **74.4%** | **1,800 GPU小时** | **9-30×** |

**关键发现**：
1. **成本效率**：On-Policy Distillation仅用RL十分之一的算力，达到更高性能
2. **FLOPs分析**：
   - 若SFT数据集已给定（成本可摊销），不计教师生成成本 → **9×成本降低**
   - 若包含教师模型完整成本（采样+logprobs计算）→ **30×成本降低**
3. **LoRA适用性**：On-Policy Distillation显著缩小了LoRA与全量微调的性能差距（从SFT后的13%缩小到蒸馏后的6%）

**机制解释**：密集的逐token反馈能够精确纠正学生在推理链中的每个错误步骤。例如，当学生错误应用二次公式时，教师模型会立即对那个错误token给予负向评分——如同导师用红笔圈出错题的具体步骤。

### 3.2 个性化助手：恢复灾难性遗忘的能力

**挑战情境**：当在特定领域数据（如公司内部文档）上进行中期训练后，模型往往会出现**灾难性遗忘**——丧失原有的指令遵循等后训练行为。

**实验设计**：
- **起点**：Qwen3-8B（经过RLHF的指令模型）
- **中期训练**：在内部文档上微调以获得领域知识
- **目标**：同时保持两种能力
  1. **知识能力**：内部QA评估（专业知识召回）
  2. **行为能力**：IF-eval（指令遵循）

**训练过程观察**：
1. **初始状态**：Qwen3-8B的IF-eval为85%，内部QA为18%
2. **中期训练后**（70%文档+30%聊天数据混合）：
   - 内部QA提升到36%（✓ 成功学到知识）
   - IF-eval降至79%（✗ 能力退化）
3. **纯文档训练更糟**（100%文档）：
   - 内部QA达到43%
   - IF-eval骤降至45%（接近崩溃）

**即使使用LoRA约束参数更新，仍无法防止能力退化。**

**On-Policy Distillation的救赎**：
使用**模型早期版本**（Qwen3-8B原版）作为教师，在Tulu3聊天数据上进行on-policy蒸馏：

| 模型状态 | 内部QA（知识） | IF-eval（聊天） |
|---------|---------------|----------------|
| Qwen3-8B（原版） | 18% | 85% |
| + 中期训练（70%文档） | 36% | 79% ↓ |
| + **On-Policy蒸馏** | **41%** ↑ | **83%** ↑ |

**惊人结果**：
- IF-eval几乎**完全恢复**（83% vs 原始85%）
- 内部知识不仅**未损失**，反而进一步提升（36%→41%）
- 出现**正向迁移**：聊天能力与知识能力互相促进

**持续学习的启示**：
这展示了on-policy蒸馏作为**持续学习工具**的潜力。可以采用"交替训练范式"：
1. **微调阶段**：在新领域数据上训练，获取新知识
2. **蒸馏阶段**：用早期checkpoint作教师，恢复原有能力
3. 循环往复，让模型不断学习新知识又不忘旧本领

**为何离策略SFT会失败**？
实验发现，即使在模型**自己生成的样本**上进行SFT，只要学习率>0，IF-eval也会退化！原因是：
- 虽然期望KL散度为0，但每个有限batch的分布略有不同
- 在这些batch上训练产生非零梯度，使更新后的模型偏离原始状态
- 这个过程将"自己样本的训练"逐渐变成"离策略训练"
- 导致与off-policy训练相同的误差累积

**On-Policy Distillation始终保持on-policy**，且教师固定，学生收敛于教师的理想行为，不会出现自蒸馏中的退化。

## 第四章：技术深度解析

### 4.1 计算效率的秘密：为何能实现50-100×加速

**直接对比实验**：
1. 起点：Qwen3-8B-Base（无SFT）
2. 用RL在DeepMath上训练（LoRA rank=128）→ 得到教师
3. 从教师on-policy蒸馏回基础模型

**结果**：On-Policy Distillation在约**7-10个梯度步**内达到RL需要70步才能达到的性能：
- AIME'24得分快速恢复
- 反向KL降至接近零

**综合成本降低50-100×**的原因：
1. **训练上下文更短**：
   - RL需要在接近评估上下文长度训练（避免格式惩罚）
   - 蒸馏可在较短上下文有效学习（无sharp reward cutoff）
   - 上下文长度节省 → 计算量大幅降低

2. **批量大小更小**：
   - SFT初始化强时，密集反馈提供更多bits/episode
   - 降低梯度噪声，可用更小batch size
   - 批量大小节省 → 内存与计算降低

3. **logprobs计算可并行**：
   - 教师logprobs计算仅需一次前向传播
   - 可在GPU上高效并行化
   - FLOPs惩罚在实际中被摊销

**信息论视角**：
- **RL**：每个episode教授$O(1)$比特信息（终局奖励）
- **Distillation**：每个episode教授$O(N)$比特信息（$N$为token数）
- 当$N\sim$数百时，信息密度提升**数百倍**

### 4.2 数据效率：能否从单个样本学习？

**极限测试**：仅用**一个随机选择的prompt**训练，连续20步，每步batch=256，共5120个打分序列。

**结果**：尽管这种多epoch训练通常导致过拟合，模型仍能**大致匹配教师的AIME'24性能**。

**为何不会过拟合记忆答案**？
On-Policy Distillation学习的是**匹配教师的完整分布**（通过最小化反向KL），而非记忆单一答案。学生学会的是：
- 在给定上下文下的推理策略
- 教师的思考方式和分布特征
- 如何生成类似教师的多样化输出

这与RL的多epoch训练形成鲜明对比——RL在大模型上多epoch容易简单记忆终局答案。

### 4.3 损失函数设计的艺术

**为何反向KL适合推理任务**？

考虑一个状态，教师认为有两个合理路径：
- 路径A：概率70%（略优）
- 路径B：概率30%（次优但可行）

**前向KL**（mean-seeking）：鼓励学生同时覆盖A和B，可能导致"50%-50%"这种模糊策略。

**反向KL**（mode-seeking）：严厉惩罚学生在教师低概率区域赋高概率。结果：
- 学生集中学习路径A（教师最推荐）
- 避免探索教师从未考虑的怪异路径
- 适合需要精确、确定性推理的任务

**实际案例**（SimpleBench）：
问题：冰块放在煎锅中，平均每分钟放置数量如何变化？
- 正确答案：B. 0（冰块会融化）
- 学生错误：当作纯数学题处理，忽略物理上下文

教师打分显示：
- 最大惩罚集中在**"forking tokens"**——将学生引向错误方向的关键token
- 如"calculate"、"average"等数学术语token
- 最终错误答案"21"反而惩罚小（因为在给定错误推理下它是可预测的）

这种精准的token级反馈，正是蒸馏高效的根源。

## 第五章：应用前景与局限性

### 5.1 On-Policy Distillation的理想应用场景

**✓ 数学与逻辑推理**
- 需要精确、多步推理
- 教师能可靠产生正确推理链
- 示例：AIME、GSM8K、科学问答

**✓ 代码生成与调试**
- 语法和语义都有明确正确性
- 教师模型（如GPT-4）在代码任务上表现强
- 可蒸馏到小模型供IDE实时补全

**✓ 领域专家系统**
- 结合领域知识与指令遵循
- 持续学习场景：定期更新知识，保持行为
- 示例：医疗笔记提取、法律文档分析

**✓ 模型对齐与安全**
- 用对齐后的大模型指导小模型
- 比RLHF成本低廉9-30倍
- 可快速迭代、A/B测试不同对齐策略

### 5.2 当前挑战与局限

**① 教师偏见与错误（Teacher Bias）**
- 学生性能上限受限于教师
- 教师的系统性错误会被忠实传递
- 学生无法通过蒸馏超越有缺陷的教师

**② 容量不匹配（Capacity Mismatch）**
- 教师-学生规模差距过大时效果下降
- 小学生模型可能无法捕捉教师的复杂模式
- 需要精心选择学生模型架构

**③ 低质量学生样本的恶性循环**
- 训练初期，学生策略弱，生成大量低质量轨迹
- 教师对"糟糕"状态的反馈可能充满噪声或不具指导性
- 可能形成：差学生→差样本→差反馈→更差学生

**④ 分词器不匹配（Tokenizer Misalignment）**
- 学生与教师使用不同分词器时，同一文本被分割成不同token序列
- 逐token概率比较变得不可能
- 需要复杂的序列对齐和词汇表对齐算法

**⑤ 计算资源需求**
- 需要强大的教师模型时刻"待命"
- 教师通常是大模型或昂贵的评估器
- 生产环境中可能需要专门的推理基础设施

### 5.3 前沿方向：下一代蒸馏技术

**① 推测式知识蒸馏（Speculative KD, SKD）**
针对"低质量学生样本"问题的创新解决方案：
- **动态师生协作**：生成每个token时，学生先提议，教师验证
- **自适应蒸馏模式**：
  - 初期：学生提议多被拒绝 → 近似监督蒸馏（避免垃圾数据）
  - 后期：学生提议多被接受 → 过渡到在策略蒸馏（分布对齐）
- **优势**：在训练全程动态调节on-policy与off-policy的平衡

**② 混合蒸馏策略（Hybrid Distillation）**
同时优化on-policy和off-policy目标：
- **对抗性矩匹配蒸馏**（NeurIPS 2024）：通过匹配动作值函数的高阶矩传递知识
- **训练流程**：学生在线生成输出（on-policy）+ 教师预生成数据（off-policy）混合训练
- **实验结果**：混合优化在多个数据集上取得最佳效果，证明两种信号的协同作用

**③ 精炼式策略蒸馏（Refined Policy Distillation, RPD）**
针对机器人学的混合方法：
- **不仅蒸馏，更要改进**：VLA教师提供初始动作建议，但学生同时通过标准RL从环境奖励学习
- **目标**：学生不仅模仿教师，还能通过自身探索发现更优策略
- **意义**：从"纯模仿学习"向"有指导的优化"转变，**超越教师成为可能**

**④ 可解释性蒸馏（Interpretable Distillation）**
- 将复杂神经网络策略蒸馏到**简单可解释模型**（线性模型、符号方程）
- 关键应用：金融、医疗等高风险领域
- 目标：为部署RL系统提供透明度和可验证性

## 第六章：实践指南与最佳实践

### 6.1 选择合适的教师模型

**规模考量**：
- 教师应显著大于学生（经验法则：4-8倍参数量）
- 但不要过大（Qwen3-32B → 8B效果好于235B → 8B）

**质量优先**：
- 教师在目标任务上的性能是学生上限
- 优先选择在目标领域表现卓越的教师
- 可以是RL训练后的模型、专家模型、或对齐后的通用模型

**可访问性**：
- 需要高效的logprobs计算接口
- 理想情况：自己部署的开源模型
- 闭源API（如GPT-4）：成本可能抵消蒸馏优势

### 6.2 训练流程最佳实践

**三阶段训练范式**：
```
预训练（Pre-training）
    ↓
中期训练（Mid-training）
    ↓ 离策略蒸馏/SFT
    ↓ 在教师数据或领域数据上
    ↓
后训练（Post-training）
    ↓ On-Policy Distillation
    ↓ 在学生自己的轨迹上
    ↓
部署
```

**超参数推荐**：
- **批量大小**：64-256 prompts/batch，每个prompt 2-4个样本
- **学习率**：比SFT略低（5e-6 vs 1e-5），使用线性衰减
- **训练步数**：通常100-200步即可见显著提升
- **采样温度**：学生用0.7-1.0保持多样性

**持续学习循环**：
```
知识更新阶段：
  在新领域数据上SFT（可能遗忘行为）
    ↓
能力恢复阶段：
  用旧checkpoint作教师，on-policy蒸馏（恢复行为）
    ↓
评估与迭代：
  检查知识+行为双重指标，循环往复
```

### 6.3 故障排除与调试

**问题：学生性能停滞不前**
- **检查**：学生样本质量、教师反馈是否有信息量
- **解决**：
  - 先做少量off-policy warm-up提升学生基线
  - 采用SKD动态调节on/off-policy比例
  - 缩小教师-学生规模差距

**问题：训练不稳定、发散**
- **检查**：学习率、KL散度趋势
- **解决**：
  - 降低学习率
  - 使用梯度裁剪
  - 增大batch size降低方差
  - 考虑使用PPO-style的KL惩罚项

**问题：学生过度拟合教师风格，缺乏泛化**
- **检查**：学生是否只是"鹦鹉学舌"
- **解决**：
  - 引入适量探索（如ε-greedy采样）
  - 混合不同教师的知识
  - 结合少量环境奖励信号

## 结语：模型训练的第三条道路

On-Policy Distillation的出现，标志着模型后训练技术的成熟。它不是简单的技术组合，而是对学习哲学的深刻理解：

- **从何学**：从学生自己的经验中学（on-policy relevance）
- **如何学**：接受教师对每一步的密集指导（dense supervision）
- **为何高效**：在最相关的数据上施加最丰富的信号

在大模型时代，成本与性能的权衡愈发关键。On-Policy Distillation以9-30倍的成本效率，达到甚至超越传统RL的性能，为以下场景开辟了新可能：

✓ **小模型专家化**：从通用大模型中"萃取"特定领域能力
✓ **持续学习系统**：不断吸收新知识又不忘旧本领
✓ **低成本对齐**：快速迭代、实验不同的对齐策略
✓ **边缘设备部署**：将云端巨模型的能力压缩到本地

展望未来，我们期待看到：
- 更智能的动态蒸馏算法（如SKD）成为主流
- 蒸馏与RL的深度融合（如RPD）
- 跨模态蒸馏的突破（视觉-语言-动作模型）
- 可解释性蒸馏在关键领域的应用

正如AlphaGo Zero证明了自我博弈的力量，On-Policy Distillation证明了**从自己的错误中学习、同时接受大师指导**这一理念的普适性。它既是知识蒸馏的自然演进，也是强化学习的高效替代，更是通向下一代智能系统的重要桥梁。

---

**关键要点总结**：
1. On-Policy Distillation = 学生自己的轨迹 + 教师逐token密集反馈
2. 反向KL的"模式寻求"特性适合精确推理任务
3. 相比RL实现9-30倍成本降低，相比SFT避免分布失配
4. 可用于持续学习：交替"学新知识"与"恢复能力"
5. 局限在于依赖高质量教师和初期学生样本质量
6. 前沿方向：推测式蒸馏（SKD）、混合蒸馏、精炼式蒸馏

*如果AI训练是一场马拉松，传统方法让你要么自己摸索（RL），要么背诵教材（SFT）。On-Policy Distillation则是：你自己跑，教练在每个路口实时指导——这才是冠军的训练方式。* 🏃‍♂️💨
